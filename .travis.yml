# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Spark provides this Travis CI configuration file to help contributors
# check Scala/Java style conformance and JDK7/8 compilation easily
# during their preparing pull requests.
#   - Scalastyle is executed during `maven install` implicitly.
#   - Java Checkstyle is executed by `lint-java`.
# See the related discussion here.
# https://github.com/apache/spark/pull/12980

# 1. Choose OS (Ubuntu 14.04.3 LTS Server Edition 64bit, ~2 CORE, 7.5GB RAM)
sudo: required
dist: trusty

# 2. Choose language and target JDKs for parallel builds.
language: scala
scala:
  - 2.11.8

# 3. Setup cache directory for SBT and Maven.
cache:
  directories:
  - $HOME/.sbt
  - $HOME/.m2
  - $HOME/.ivy2

# 4. Turn off notifications.
notifications:
  email: yaooqinn@hotmail.com

before_install:
  - unset SBT_OPTS JVM_OPTS
  - wget -P $HOME/.m2/repository/yaooqinn/spark-authorizer/1.1.3.spark2.1 http://dl.bintray.com/spark-packages/maven/yaooqinn/spark-authorizer/1.1.3.spark2.1/spark-authorizer-1.1.3.spark2.1.jar
  - wget -P $HOME/.m2/repository/yaooqinn/spark-authorizer/1.1.3.spark2.1 http://dl.bintray.com/spark-packages/maven/yaooqinn/spark-authorizer/1.1.3.spark2.1/spark-authorizer-1.1.3.spark2.1.pom

install:
  - build/sbt -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Pkafka-0-8 -Phive assembly/package test:package | grep -v  "^.*[info].*Resolving"

# 6. Run test

jobs:
  include:
    - stage: core-java-scala-test
      script: ./build/sbt -mem 4096 -Phadoop-2.7 core/testOnly  org.apache.spark.scheduler.SparkListenerWithClusterSuite
#    - stage: core-java-scala-test
#      script: ./build/sbt -mem 4096 -Phadoop-2.7 core/testOnly  test.org.apache.spark.* org.apache.sparktest.*
    - stage: core-a-m-test
      script: ./build/sbt -mem 4096 -Phadoop-2.7 core/testOnly   org.apache.spark.a*  org.apache.spark.b* org.apache.spark.c* org.apache.spark.d* org.apache.spark.e* org.apache.spark.f* org.apache.spark.g* org.apache.spark.h* org.apache.spark.i* org.apache.spark.j* org.apache.spark.k* org.apache.spark.l* org.apache.spark.m*
    - stage: core-n-z-test
      script: ./build/sbt -mem 4096 -Phadoop-2.7 core/testOnly   org.apache.spark.n*  org.apache.spark.o* org.apache.spark.p* org.apache.spark.q* org.apache.spark.r* org.apache.spark.s* org.apache.spark.t* org.apache.spark.u* org.apache.spark.v* org.apache.spark.w* org.apache.spark.x* org.apache.spark.y* org.apache.spark.z*
    - stage: core-A-Z-test
      script: ./build/sbt -mem 4096 -Phadoop-2.7 core/testOnly   org.apache.spark.A*  org.apache.spark.B* org.apache.spark.C* org.apache.spark.D* org.apache.spark.E* org.apache.spark.F* org.apache.spark.G* org.apache.spark.H* org.apache.spark.I* org.apache.spark.J* org.apache.spark.K* org.apache.spark.L* org.apache.spark.M* org.apache.spark.N*  org.apache.spark.O* org.apache.spark.P* org.apache.spark.Q* org.apache.spark.R* org.apache.spark.S* org.apache.spark.T* org.apache.spark.U* org.apache.spark.V* org.apache.spark.W* org.apache.spark.X* org.apache.spark.Y* org.apache.spark.Z*
    - stage: catalyst
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive catalyst/test
    - stage: sql
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive sql/test
    - stage: hive
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive -Dtest.exclude.tags=org.apache.spark.tags.ExtendedHiveTest "hive/testOnly org.apache.spark.sql.hive.C* org.apache.spark.sql.hive.E* org.apache.spark.sql.hive.H* org.apache.spark.sql.hive.L* org.apache.spark.sql.hive.M* org.apache.spark.sql.hive.P* org.apache.spark.sql.hive.Q* org.apache.spark.sql.hive.H* org.apache.spark.sql.hive.p* org.apache.spark.sql.hive.S* org.apache.spark.sql.hive.I* org.apache.spark.sql.hive.U*"
    - stage: hive-catalyst
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive -Dtest.exclude.tags=org.apache.spark.tags.ExtendedHiveTest "hive/testOnly org.apache.spark.sql.catalyst.*"
    - stage: hive-client
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive -Dtest.exclude.tags=org.apache.spark.tags.ExtendedHiveTest "hive/testOnly org.apache.spark.sql.hive.client.*"
    - stage: hive-orc
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive -Dtest.exclude.tags=org.apache.spark.tags.ExtendedHiveTest "hive/testOnly org.apache.spark.sql.hive.orc.*"
    - stage: hive-execution
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive -Dtest.exclude.tags=org.apache.spark.tags.ExtendedHiveTest "hive/testOnly org.apache.spark.sql.hive.execution.*"
    - stage: hive-sources
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive -Dtest.exclude.tags=org.apache.spark.tags.ExtendedHiveTest "hive/testOnly org.apache.spark.sql.sources.*"
    - stage: streaming
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive streaming/test
    - stage: ml
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive mllib/test
    - stage: graphx
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive graphx/test
    - stage: network
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive network-common/test
    - stage: shuffle
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive network-shuffle/test
    - stage: unsafe
      script: ./build/sbt -mem 4096 -Phadoop-2.7 -Dhadoop.version=2.7.3 -Pyarn -Phive unsafe/test
